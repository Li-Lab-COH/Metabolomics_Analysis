{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "043324e3-280b-4bea-b210-28a1018ffff5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================\n",
    "# Enrichment pipeline (ORA)\n",
    "# =========================\n",
    "from __future__ import annotations\n",
    "import re\n",
    "from pathlib import Path\n",
    "from textwrap import wrap\n",
    "from typing import Dict, Iterable, List, Mapping, Optional, Sequence, Set, Tuple\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.stats import hypergeom\n",
    "from statsmodels.stats.multitest import multipletests\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "# -------- Configuration --------\n",
    "CFG = {\n",
    "    # Single source of truth for labels\n",
    "    \"TP_LABEL\": {\"B0\": \"Baseline\", \"D1\": \"Day 1\", \"D5\": \"Day 5\", \"F3\": \"Month 3\", \"F6\": \"Month 6\"},\n",
    "    \"TP_ORDER\": [\"B0\", \"D1\", \"D5\", \"F3\", \"F6\"],\n",
    "\n",
    "    # Column config\n",
    "    \"feature_col\": \"feature_id\",   # <-- change to your metabolite/feature ID column\n",
    "    \"id_col\": \"kegg_maps\",         # <-- pathway membership column with multiple IDs per row\n",
    "    \"id_delims\": r\"[,\\s;]+\",       # split on comma/space/semicolon\n",
    "    \"sig_alpha\": 0.05,             # default significance threshold\n",
    "\n",
    "    # Output\n",
    "    \"out_dir\": \"./results/enrichment\",  # base output dir (will create per-TP subdirs)\n",
    "\n",
    "    # Plot\n",
    "    \"plot_topn\": 20,\n",
    "    \"plot_alpha_line_is_fdr\": True,  # vline at -log10(FDR alpha) if True else P-value alpha\n",
    "}\n",
    "\n",
    "# If you have a dict mapping {pathway_id: pathway_name}, set it here, else leave as {}.\n",
    "PATHWAY_NAME_MAP: Dict[str, str] = {}\n",
    "\n",
    "\n",
    "# -------- File discovery & timepoint mapping --------\n",
    "TP_REGEX = re.compile(r\"\\b(B0|D1|D5|F3|F6)\\b\", flags=re.IGNORECASE)\n",
    "\n",
    "def parse_tp_from_name(path: Path) -> Optional[str]:\n",
    "    m = TP_REGEX.search(path.name)\n",
    "    return m.group(1).upper() if m else None\n",
    "\n",
    "def build_tp_map(files: Sequence[str]) -> Dict[str, Path]:\n",
    "    by_tp: Dict[str, Path] = {}\n",
    "    for f in files:\n",
    "        p = Path(f)\n",
    "        tp = parse_tp_from_name(p)\n",
    "        if not tp:\n",
    "            continue\n",
    "        if tp in by_tp:\n",
    "            raise ValueError(f\"Duplicate file for {tp}:\\n  {by_tp[tp]}\\n  {p}\")\n",
    "        by_tp[tp] = p\n",
    "    missing = [t for t in CFG[\"TP_LABEL\"] if t not in by_tp]\n",
    "    if missing:\n",
    "        raise ValueError(f\"Missing required timepoints in files: {missing}\")\n",
    "    return by_tp\n",
    "\n",
    "def load_tp_df(tp: str, tp_map: Mapping[str, Path]) -> Tuple[pd.DataFrame, str, Path]:\n",
    "    tp = tp.upper()\n",
    "    if tp not in CFG[\"TP_LABEL\"]:\n",
    "        raise ValueError(f\"Unsupported tp={tp}. Allowed: {list(CFG['TP_LABEL'])}\")\n",
    "    src = tp_map[tp]\n",
    "    df = pd.read_csv(src)\n",
    "    # Optional sanity check if file contains a timepoint column\n",
    "    if \"timepoint\" in df.columns:\n",
    "        tp_in_file = str(df[\"timepoint\"].iloc[0]).upper()\n",
    "        if tp_in_file != tp:\n",
    "            raise AssertionError(f\"File {src} says timepoint={tp_in_file}, expected {tp}\")\n",
    "    day = CFG[\"TP_LABEL\"][tp]\n",
    "    return df, day, src\n",
    "\n",
    "\n",
    "# -------- Helpers for pathway membership & significance --------\n",
    "def split_ids(cell: object, pattern: str = CFG[\"id_delims\"]) -> List[str]:\n",
    "    if pd.isna(cell):\n",
    "        return []\n",
    "    s = str(cell).strip()\n",
    "    if not s:\n",
    "        return []\n",
    "    return [tok for tok in re.split(pattern, s) if tok]\n",
    "\n",
    "def extract_feature_to_sets(df: pd.DataFrame,\n",
    "                            feature_col: str,\n",
    "                            id_col: str) -> Dict[str, Set[str]]:\n",
    "    if feature_col not in df.columns:\n",
    "        raise KeyError(f\"feature_col '{feature_col}' not in df\")\n",
    "    if id_col not in df.columns:\n",
    "        raise KeyError(f\"id_col '{id_col}' not in df\")\n",
    "    mapping: Dict[str, Set[str]] = {}\n",
    "    for feat, memberships in zip(df[feature_col].astype(str), df[id_col].map(split_ids)):\n",
    "        if not memberships:\n",
    "            continue\n",
    "        mapping.setdefault(feat, set()).update(memberships)\n",
    "    return mapping\n",
    "\n",
    "def infer_significant_mask(df: pd.DataFrame,\n",
    "                           alpha: float = CFG[\"sig_alpha\"],\n",
    "                           sig_flag_col: Optional[str] = None,\n",
    "                           fdr_cols: Sequence[str] = (\"FDR\",\"qval\",\"adj_pval\",\"padj\",\"BH_FDR\",\"q\",\"q_value\",\"adj.P.Val\"),\n",
    "                           p_cols: Sequence[str] = (\"pval\",\"p_value\",\"P-value\",\"P Value\",\"p\",\"P\",\"P.value\")) -> pd.Series:\n",
    "    n = len(df)\n",
    "    if sig_flag_col and sig_flag_col in df.columns:\n",
    "        mask = df[sig_flag_col].astype(bool)\n",
    "        return mask.fillna(False)\n",
    "\n",
    "    for c in fdr_cols:\n",
    "        if c in df.columns:\n",
    "            return (pd.to_numeric(df[c], errors=\"coerce\") <= alpha).fillna(False)\n",
    "\n",
    "    for c in p_cols:\n",
    "        if c in df.columns:\n",
    "            return (pd.to_numeric(df[c], errors=\"coerce\") <= alpha).fillna(False)\n",
    "\n",
    "    raise KeyError(\"Could not infer significance. Provide either a boolean flag column, \"\n",
    "                   \"or an FDR/adjusted p-value column, or a raw p-value column.\")\n",
    "\n",
    "\n",
    "# -------- Build global universe across ALL timepoints --------\n",
    "def build_global_universe(tp_map: Mapping[str, Path],\n",
    "                          feature_col: str,\n",
    "                          id_col: str) -> Tuple[Set[str], Dict[str, Set[str]]]:\n",
    "    \"\"\"\n",
    "    Returns:\n",
    "      U: set of all features observed with >=1 pathway membership across all timepoints\n",
    "      feat2sets_all: merged mapping feature -> set of pathway_ids (union across TPs)\n",
    "    \"\"\"\n",
    "    feat2sets_all: Dict[str, Set[str]] = {}\n",
    "    for tp in CFG[\"TP_ORDER\"]:\n",
    "        df, _, _ = load_tp_df(tp, tp_map)\n",
    "        m = extract_feature_to_sets(df, feature_col, id_col)\n",
    "        for feat, sets_ in m.items():\n",
    "            feat2sets_all.setdefault(feat, set()).update(sets_)\n",
    "    U = {f for f, s in feat2sets_all.items() if len(s) > 0}\n",
    "    if not U:\n",
    "        raise ValueError(\"Empty universe: no features with pathway membership across files.\")\n",
    "    return U, feat2sets_all\n",
    "\n",
    "\n",
    "# -------- ORA core --------\n",
    "def compute_ora_for_tp(df: pd.DataFrame,\n",
    "                       tp: str,\n",
    "                       universe_feats: Set[str],\n",
    "                       feat2sets_all: Mapping[str, Set[str]],\n",
    "                       feature_col: str,\n",
    "                       alpha: float = CFG[\"sig_alpha\"],\n",
    "                       sig_flag_col: Optional[str] = None) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Hypergeometric over-representation analysis using a fixed global universe.\n",
    "    \"\"\"\n",
    "    # Significant features at this TP\n",
    "    sig_mask = infer_significant_mask(df, alpha=alpha, sig_flag_col=sig_flag_col)\n",
    "    sig_feats_raw = set(df.loc[sig_mask, feature_col].astype(str))\n",
    "    # Restrict to features that are in the universe & have at least one pathway in feat2sets_all\n",
    "    sig_feats = {f for f in sig_feats_raw if f in universe_feats and f in feat2sets_all}\n",
    "\n",
    "    # Build pathway -> members (from global mapping)\n",
    "    pathway_to_members: Dict[str, Set[str]] = {}\n",
    "    for feat, sets_ in feat2sets_all.items():\n",
    "        if feat not in universe_feats:\n",
    "            continue\n",
    "        for pid in sets_:\n",
    "            pathway_to_members.setdefault(pid, set()).add(feat)\n",
    "\n",
    "    M = len(universe_feats)     # population size\n",
    "    N = len(sig_feats)          # number of draws (significant)\n",
    "    if N == 0:\n",
    "        return pd.DataFrame(columns=[\n",
    "            \"tp\",\"pathway_id\",\"pathway_name\",\"Overlap\",\"k\",\"n\",\"N\",\"M\",\n",
    "            \"Odds Ratio\",\"P-value\",\"FDR\",\"neglog10p\"\n",
    "        ])\n",
    "\n",
    "    rows = []\n",
    "    for pid, members in pathway_to_members.items():\n",
    "        n = len(members)             # pathway size in universe\n",
    "        if n == 0:\n",
    "            continue\n",
    "        k = len(sig_feats & members) # overlap\n",
    "        # hypergeom survival function: P[X >= k]\n",
    "        pval = hypergeom.sf(k - 1, M, n, N)\n",
    "        # Odds ratio with Haldane-Anscombe correction to avoid zeros\n",
    "        a = k + 0.5\n",
    "        b = (N - k) + 0.5\n",
    "        c = (n - k) + 0.5\n",
    "        d = (M - N - (n - k)) + 0.5\n",
    "        or_est = (a * d) / (b * c)\n",
    "        rows.append({\n",
    "            \"tp\": tp,\n",
    "            \"pathway_id\": pid,\n",
    "            \"pathway_name\": PATHWAY_NAME_MAP.get(pid, pid),\n",
    "            \"Overlap\": f\"{k}/{n}\",\n",
    "            \"k\": k, \"n\": n, \"N\": N, \"M\": M,\n",
    "            \"Odds Ratio\": or_est,\n",
    "            \"P-value\": pval,\n",
    "        })\n",
    "\n",
    "    res = pd.DataFrame(rows)\n",
    "    if not res.empty:\n",
    "        # Multiple testing correction across all pathways for this TP\n",
    "        res[\"FDR\"] = multipletests(res[\"P-value\"].values, method=\"fdr_bh\")[1]\n",
    "        res[\"neglog10p\"] = -np.log10(np.clip(res[\"P-value\"].values, 1e-300, 1.0))\n",
    "        res = res.sort_values([\"FDR\",\"P-value\",\"Odds Ratio\",\"pathway_id\"]).reset_index(drop=True)\n",
    "    return res\n",
    "\n",
    "\n",
    "# -------- Plotting --------\n",
    "def plot_top(df: pd.DataFrame, title: str, out_png: Path, topn: int = CFG[\"plot_topn\"],\n",
    "             alpha: float = CFG[\"sig_alpha\"], use_fdr: bool = CFG[\"plot_alpha_line_is_fdr\"]) -> None:\n",
    "    if df.empty:\n",
    "        print(f\"No enriched pathways to plot for: {title}\")\n",
    "        return\n",
    "    df = df.sort_values([\"P-value\",\"pathway_name\"]).head(topn).copy()\n",
    "    df[\"label\"] = df[\"pathway_name\"].map(lambda s: \"\\n\".join(wrap(str(s), width=35)))\n",
    "\n",
    "    plt.figure(figsize=(9, max(4, 0.45*len(df))))\n",
    "    y = np.arange(len(df))[::-1]\n",
    "    plt.barh(y, df[\"neglog10p\"].values)\n",
    "    plt.yticks(y, df[\"label\"].values)\n",
    "    plt.xlabel(r\"$-\\log_{10}(\\mathrm{P\\text{-}value})$\")\n",
    "    plt.title(title)\n",
    "\n",
    "    # Add text labels (OR and Overlap)\n",
    "    for i, (xv, orr, ov) in enumerate(zip(df[\"neglog10p\"], df[\"Odds Ratio\"], df[\"Overlap\"])):\n",
    "        plt.text(xv + 0.05, y[i], f\"OR={orr:.2f} | {ov}\", va=\"center\", fontsize=9)\n",
    "\n",
    "    # Vertical line for significance threshold\n",
    "    thresh = -np.log10(alpha)\n",
    "    if use_fdr:\n",
    "        # If using FDR as the decision criterion, convert: we don't know per-row FDR<alpha → bar heights are -log10(P)\n",
    "        # The most interpretable is still a P-value line unless you also plot -log10(FDR).\n",
    "        # If you wish to draw a line at FDR alpha, consider plotting -log10(FDR) instead.\n",
    "        pass\n",
    "    plt.axvline(thresh, linestyle=\"--\", linewidth=1.2)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    out_png.parent.mkdir(parents=True, exist_ok=True)\n",
    "    plt.savefig(out_png, dpi=200, bbox_inches=\"tight\")\n",
    "    plt.close()\n",
    "\n",
    "\n",
    "# -------- Orchestration --------\n",
    "def run_for_timepoint(tp: str,\n",
    "                      files: Sequence[str],\n",
    "                      out_base: str = CFG[\"out_dir\"],\n",
    "                      feature_col: str = CFG[\"feature_col\"],\n",
    "                      id_col: str = CFG[\"id_col\"],\n",
    "                      alpha: float = CFG[\"sig_alpha\"],\n",
    "                      sig_flag_col: Optional[str] = None,\n",
    "                      make_plot: bool = True) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Deterministic load + global-universe ORA for a single timepoint.\n",
    "    Writes CSV/Parquet/PNG under out_base/{tp}/\n",
    "    \"\"\"\n",
    "    tp_map = build_tp_map(files)\n",
    "    # Build global universe once using all files\n",
    "    U, feat2sets_all = build_global_universe(tp_map, feature_col, id_col)\n",
    "    df_tp, day_label, src = load_tp_df(tp, tp_map)\n",
    "\n",
    "    # ORA\n",
    "    res = compute_ora_for_tp(df_tp, tp, U, feat2sets_all, feature_col, alpha=alpha, sig_flag_col=sig_flag_col)\n",
    "\n",
    "    # Write outputs\n",
    "    out_dir = Path(out_base) / tp\n",
    "    out_dir.mkdir(parents=True, exist_ok=True)\n",
    "    out_csv = out_dir / f\"enrichment_{tp}.csv\"\n",
    "    out_parq = out_dir / f\"enrichment_{tp}.parquet\"\n",
    "    res.to_csv(out_csv, index=False)\n",
    "    try:\n",
    "        res.to_parquet(out_parq, index=False)\n",
    "    except Exception:\n",
    "        pass  # parquet optional\n",
    "\n",
    "    # Plot\n",
    "    if make_plot and not res.empty:\n",
    "        out_png = out_dir / f\"enrichment_{tp}.png\"\n",
    "        plot_top(res, f\"{tp} — {day_label}\", out_png, alpha=alpha)\n",
    "\n",
    "    print(f\"[{tp}] Loaded from: {src}\")\n",
    "    print(f\"[{tp}] Universe size M={len(U)}; N_sig={res['N'].iloc[0] if not res.empty else 0}\")\n",
    "    print(f\"[{tp}] Wrote: {out_csv}\")\n",
    "    return res\n",
    "\n",
    "def run_all_timepoints(files: Sequence[str],\n",
    "                       out_base: str = CFG[\"out_dir\"],\n",
    "                       feature_col: str = CFG[\"feature_col\"],\n",
    "                       id_col: str = CFG[\"id_col\"],\n",
    "                       alpha: float = CFG[\"sig_alpha\"],\n",
    "                       sig_flag_col: Optional[str] = None,\n",
    "                       make_plots: bool = True) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Iterate in canonical order with a single, shared universe across all TPs.\n",
    "    Returns concatenated results with a 'tp' column.\n",
    "    \"\"\"\n",
    "    tp_map = build_tp_map(files)\n",
    "    U, feat2sets_all = build_global_universe(tp_map, feature_col, id_col)\n",
    "    all_res: List[pd.DataFrame] = []\n",
    "\n",
    "    for tp in CFG[\"TP_ORDER\"]:\n",
    "        df_tp, day_label, src = load_tp_df(tp, tp_map)\n",
    "        res = compute_ora_for_tp(df_tp, tp, U, feat2sets_all, feature_col, alpha=alpha, sig_flag_col=sig_flag_col)\n",
    "        out_dir = Path(out_base) / tp\n",
    "        out_dir.mkdir(parents=True, exist_ok=True)\n",
    "        res.to_csv(out_dir / f\"enrichment_{tp}.csv\", index=False)\n",
    "        try:\n",
    "            res.to_parquet(out_dir / f\"enrichment_{tp}.parquet\", index=False)\n",
    "        except Exception:\n",
    "            pass\n",
    "        if make_plots and not res.empty:\n",
    "            plot_top(res, f\"{tp} — {CFG['TP_LABEL'][tp]}\", out_dir / f\"enrichment_{tp}.png\", alpha=alpha)\n",
    "        print(f\"[{tp}] from {src}: M={len(U)}, N_sig={res['N'].iloc[0] if not res.empty else 0}, results={len(res)}\")\n",
    "        all_res.append(res)\n",
    "\n",
    "    out_all = Path(out_base) / \"combined_enrichment.csv\"\n",
    "    pd.concat(all_res, ignore_index=True).to_csv(out_all, index=False)\n",
    "    print(f\"Wrote combined: {out_all}\")\n",
    "    return pd.concat(all_res, ignore_index=True)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
